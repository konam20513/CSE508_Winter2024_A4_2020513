{"cells":[{"cell_type":"markdown","metadata":{},"source":["***Training***"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T04:41:25.445053Z","iopub.status.busy":"2024-04-22T04:41:25.444654Z","iopub.status.idle":"2024-04-22T04:41:32.227116Z","shell.execute_reply":"2024-04-22T04:41:32.226243Z","shell.execute_reply.started":"2024-04-22T04:41:25.445018Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","df=pd.read_csv('/kaggle/input/akhil-ir-a4/preprocessed_data.csv')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T04:41:32.229806Z","iopub.status.busy":"2024-04-22T04:41:32.229102Z","iopub.status.idle":"2024-04-22T04:41:32.311767Z","shell.execute_reply":"2024-04-22T04:41:32.310623Z","shell.execute_reply.started":"2024-04-22T04:41:32.229765Z"},"trusted":true},"outputs":[],"source":["df = df.sample(n=50000, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T04:41:32.319516Z","iopub.status.busy":"2024-04-22T04:41:32.319135Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","\n","# Initialize the tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","class ReviewSummaryDataset(Dataset):\n","    def __init__(self, tokenizer, reviews, summaries, max_length):\n","        self.tokenizer = tokenizer\n","        self.reviews = reviews\n","        self.summaries = summaries\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.reviews)\n","    \n","    def __getitem__(self, idx):\n","        review = self.reviews[idx]\n","        summary = self.summaries[idx]\n","        encodings = self.tokenizer.encode_plus(\n","            review, summary,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","        labels = encodings.input_ids.detach().clone()\n","        labels[labels == tokenizer.pad_token_id] = -100\n","        return {\"input_ids\": encodings.input_ids.squeeze(), \"attention_mask\": encodings.attention_mask.squeeze(), \"labels\": labels.squeeze()}\n","\n","\n","# Remove rows with missing values\n","df = df.dropna()\n","\n","# Split dataset\n","train_texts, val_texts, train_labels, val_labels = train_test_split(df['Text'], df['Summary'], test_size=0.25)\n","\n","# Initialize dataset\n","train_dataset = ReviewSummaryDataset(tokenizer, train_texts.to_list(), train_labels.to_list(), max_length=512)\n","val_dataset = ReviewSummaryDataset(tokenizer, val_texts.to_list(), val_labels.to_list(), max_length=512)\n","\n","# Initialize the model\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=10,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    save_strategy=\"no\",  \n","    logging_strategy=\"no\",  \n","    warmup_steps=500,\n","    weight_decay=0.01,\n",")\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Save the model\n","model_path = \"saved_model\"\n","model.save_pretrained(model_path)\n"]},{"cell_type":"markdown","metadata":{},"source":["***Evaluation***"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","df=pd.read_csv('preprocessed_data.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["df = df.sample(n=50000, random_state=42)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Remove rows with missing values\n","df = df.dropna()\n","\n","# Split dataset\n","train_texts, val_texts, train_labels, val_labels = train_test_split(df['Text'], df['Summary'], test_size=0.25)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","import pandas as pd\n","from rouge_score import rouge_scorer\n","\n","# Load the trained model and tokenizer\n","model_path = \"model_kaggle\"\n","model = GPT2LMHeadModel.from_pretrained(model_path)\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='left')  # Adjust padding\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Setup device for PyTorch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","def generate_summary(text):\n","    # Encode text, ensuring padding is correctly managed\n","    encodings = tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        max_length=512,\n","        return_tensors=\"pt\",\n","        padding=\"max_length\",\n","        truncation=True\n","    )\n","    inputs = encodings['input_ids'].to(device)\n","    attention_mask = encodings['attention_mask'].to(device)\n","\n","    # Correct handling of generation parameters\n","    max_new_tokens = 25  # Specify how many tokens to generate at most\n","    outputs = model.generate(\n","        inputs,\n","        attention_mask=attention_mask,\n","        max_new_tokens=max_new_tokens,  # Use max_new_tokens instead of max_length\n","        num_beams=5,\n","        early_stopping=True\n","    )\n","    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return summary\n","\n","# Compute ROUGE scores and prepare CSV data\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","results = []\n","\n","# Iterate over both texts and labels\n","for text, actual_summary in zip(val_texts, val_labels):\n","    generated_summary = generate_summary(text)\n","    scores = scorer.score(actual_summary, generated_summary)\n","\n","    result = {\n","        \"Text\": text,\n","        \"ROUGE-1 Precision\": scores['rouge1'].precision,\n","        \"ROUGE-1 Recall\": scores['rouge1'].recall,\n","        \"ROUGE-1 F1\": scores['rouge1'].fmeasure,\n","        \"ROUGE-2 Precision\": scores['rouge2'].precision,\n","        \"ROUGE-2 Recall\": scores['rouge2'].recall,\n","        \"ROUGE-2 F1\": scores['rouge2'].fmeasure,\n","        \"ROUGE-L Precision\": scores['rougeL'].precision,\n","        \"ROUGE-L Recall\": scores['rougeL'].recall,\n","        \"ROUGE-L F1\": scores['rougeL'].fmeasure\n","    }\n","    results.append(result)\n","\n","# Save results to CSV\n","results_df = pd.DataFrame(results)\n","results_df.to_csv('summary_rouge_scores.csv', index=False)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4837336,"sourceId":8172961,"sourceType":"datasetVersion"},{"datasetId":4845305,"sourceId":8183413,"sourceType":"datasetVersion"},{"datasetId":4847328,"sourceId":8186238,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
